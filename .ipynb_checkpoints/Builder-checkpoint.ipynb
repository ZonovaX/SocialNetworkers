{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95c4e16",
   "metadata": {},
   "source": [
    "# Use this on a Twitter account to data about the activity of a network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c1ccc",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5eec9d",
   "metadata": {},
   "source": [
    "### Import these packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985a0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy\n",
    "import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f76ca3",
   "metadata": {},
   "source": [
    "### Choose your parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d9f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Screen name of network's center\n",
    "center = 'erdosinstitute'\n",
    "# Name to save files to\n",
    "name = 'Test'\n",
    "try: os.mkdir('./'+name)\n",
    "except: pass\n",
    "# Number of mutuals to find\n",
    "breadth = 2\n",
    "# Number of times to find them\n",
    "depth = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1545e4",
   "metadata": {},
   "source": [
    "### Choose your senitment analyzer. For example here we use Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3397fc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Ysgard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Necessary parts of the analyzer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# The analyzer itself\n",
    "def analyzer(tweet):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(tweet)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "927ad589",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We could also use this\n",
    "#from joblib import load\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#import string\n",
    "#from nltk.stem import PorterStemmer\n",
    "#import re\n",
    "#\n",
    "#stop_words=stopwords.words('english')\n",
    "#punct=string.punctuation\n",
    "#stemmer=PorterStemmer()\n",
    "#\n",
    "#vectord = load('vectord.joblib') \n",
    "#model = load('model.joblib') \n",
    "#\n",
    "#def analyzer(tweet):\n",
    "#    #clean the text\n",
    "#    #this removes mentions\n",
    "#    twittre = re.sub(r'@[A-Za-z0-9_]+', '', tweet)\n",
    "#    #this removes urls\n",
    "#    twittre = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', twittre)        \n",
    "#    #this removes everything but letters from the str\n",
    "#    twittre = re.sub('[^a-zA-Z]',' ',twittre)\n",
    "#    #this makes everything lowercase, then turns everything that is separated by a space into its own str\n",
    "#    twittre = twittre.lower().split()\n",
    "#    #this removes stop words and stems everything else\n",
    "#    twittre = [stemmer.stem(word) for word in twittre if (word not in stop_words)]\n",
    "#    # this puts it all back together with spaces inbetween\n",
    "#    twittre = ' '.join(twittre)\n",
    "#    activity.['Cleaned'] = cleets\n",
    "#\n",
    "#    probs = model.predict_proba(vectord.transform(tweet).toarray())\n",
    "#    return probs[:,1]-probs[:,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb2aa8",
   "metadata": {},
   "source": [
    "### You'll need a Twitter developer account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a7fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your credentials\n",
    "API_key = ''\n",
    "API_secret_key = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "# Connect to the API\n",
    "authenticator = tweepy.OAuthHandler(API_key, API_secret_key)\n",
    "authenticator.set_access_token(access_token, access_token_secret)\n",
    "API = tweepy.API(authenticator, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd046acd",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf2b93",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895a17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(user, breadth=10, depth=2, network=[], save=False):\n",
    "    #type user as a screen name so this works\n",
    "    howrite = 'a'\n",
    "    \n",
    "    # Make sure we're finding the right user\n",
    "    if type(user)==str:\n",
    "        user = API.get_user(screen_name=user).id\n",
    "        howrite = 'w'\n",
    "    elif type(user)!=int:\n",
    "        raise Exception(\"Bad user.\")\n",
    "    \n",
    "    #make sure we have good data\n",
    "    try:\n",
    "        friends = API.friends_ids(user_id=user)\n",
    "        friends[0]\n",
    "    except: \n",
    "        return False\n",
    "    \n",
    "    #one list for user's mutuals, one list for everyone\n",
    "    mutuals = []\n",
    "    network.append(user)\n",
    "    if save:\n",
    "        try:\n",
    "            with open(save+'.txt', howrite) as save_file:\n",
    "                save_file.write(str(user)+'\\n')\n",
    "        except: \n",
    "            raise Exception('Bad save file')\n",
    "    \n",
    "    #we won't delve too deep or too greedily\n",
    "    if depth:\n",
    "        #find random friends of user who follow user back (mutuals)\n",
    "        for friend in np.random.permutation(friends):\n",
    "            friend = int(friend)\n",
    "            if friend not in network:\n",
    "                if API.show_friendship(source_id=friend, target_id=user)[0].following:\n",
    "                    #find their mutuals, but only add them to the list if it's good data\n",
    "                    if build_network(friend, breadth=breadth, depth=depth-1, network=network, save=save):\n",
    "                        #add them to the list\n",
    "                        mutuals.append(friend)\n",
    "                    #but only some of them\n",
    "                    if len(mutuals)==breadth:\n",
    "                        print(friend,'has mutuals',mutuals)\n",
    "                        break\n",
    "    \n",
    "    #and here's the list\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcc73fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(user, count=200,\n",
    "                 streak_lengths=[datetime.timedelta(minutes=30),datetime.timedelta(hours=6)], \n",
    "                 per_periods=[datetime.timedelta(hours=1),datetime.timedelta(days=1)], save=False):\n",
    "    # Make sure we're finding the right user\n",
    "    if type(user)==str:\n",
    "        apitivity = API.user_timeline(screen_name=user, count=count)\n",
    "    elif type(user)==int:\n",
    "        apitivity = API.user_timeline(user_id=user, count=count)\n",
    "    else:\n",
    "        raise Exception(\"Bad user.\") \n",
    "        \n",
    "    # Make a list of the statuses\n",
    "    activiray = []\n",
    "    #and add the relevant data\n",
    "    for tweet in apitivity:\n",
    "        try: \n",
    "            tweet.retweeted_status\n",
    "            is_retweet = True\n",
    "        except: \n",
    "            is_retweet = False\n",
    "        activiray.append([tweet.text,\n",
    "                         tweet.id,\n",
    "                         is_retweet,\n",
    "                         tweet.is_quote_status,\n",
    "                         type(tweet.in_reply_to_status_id)==int,\n",
    "                         tweet.retweet_count,\n",
    "                         tweet.favorite_count,\n",
    "                         tweet.created_at])\n",
    "        \n",
    "    # turn it into a dataframe\n",
    "    activity = pd.DataFrame(activiray, \n",
    "                            columns=['Text', 'Id', 'Is_retweet', 'Is_quote', 'Is_reply', 'Retweets', 'Favorites', 'Created'])\n",
    "    \n",
    "    # record the time since the last status update\n",
    "    tsl = [activity.Created[n].to_pydatetime() - activity.Created[n+1].to_pydatetime()\n",
    "                                   for n in range(len(activity)-1)] + [np.nan]\n",
    "    activity['Time_since_last'] = tsl.copy()\n",
    "    \n",
    "    #record how many times in a row the last activity was less than length \n",
    "    for length in streak_lengths:\n",
    "        #record when it was less than length\n",
    "        streak_break = [time>length for time in tsl[:-1]]+[True]\n",
    "        \n",
    "        #intialize beginning of a streak\n",
    "        start = 0\n",
    "        streak = []\n",
    "        for end in range(len(streak_break)):\n",
    "            #if this is the end of a streak,  \n",
    "            if streak_break[end]:\n",
    "                #record it,\n",
    "                streak += [end+1-start for indx in range(end+1-start)]\n",
    "                #and begin the next\n",
    "                start = end+1\n",
    "        \n",
    "        #finally, add it to the data frame\n",
    "        activity['Streak of '+str(length)] = streak\n",
    "        \n",
    "    #record how much activity occured within a given period \n",
    "    for period in per_periods:\n",
    "        per = []\n",
    "        for n in range(len(tsl)):\n",
    "            elapsed = tsl[n]\n",
    "            count, m = 0, 1\n",
    "            #first we look at earlier tweets\n",
    "            while (m+n<len(tsl)) and (elapsed<=period/2):\n",
    "                count += 1\n",
    "                try: elapsed += tsl[m+n]\n",
    "                except: pass\n",
    "                m += 1\n",
    "            m = 0\n",
    "            elapsed = datetime.timedelta(0)\n",
    "            #then later tweets\n",
    "            while n>=m and elapsed<=period/2:\n",
    "                count += 1\n",
    "                m += 1\n",
    "                try: elapsed += tsl[n-m]\n",
    "                except: pass\n",
    "            per += [count]\n",
    "        activity['Per '+str(period)] = per\n",
    "        \n",
    "    if save:\n",
    "        activity.to_json(save, index=False)\n",
    "        \n",
    "    return activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "588bfe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_activities(network, activities=pd.DataFrame(), save=False):    \n",
    "    #concatenate each \n",
    "    for n, user in enumerate(network):\n",
    "        try:\n",
    "            activity = get_activity(user, count=1000)\n",
    "            activity['Predicted_polarity'] = [analyzer(text) for text in activity.Text]\n",
    "            activity['User'] = [user for n in range(len(activity))]\n",
    "            activities = pd.concat([activities, activity])\n",
    "        except: continue\n",
    "        if not n%10: print('Finished', n)\n",
    "    \n",
    "    #reindex because concatenation screwed it up\n",
    "    activities.index = range(len(activities.index))\n",
    "    \n",
    "    if save:\n",
    "        activities.to_json(save+'_full.json')\n",
    "        activities[['Is_retweet','Is_quote','Is_reply','Retweets','Favorites','Created','Time_since_last','Streak of 0:30:00','Streak of 6:00:00','Per 1:00:00',\"Per 1 day, 0:00:00\",'Predicted_polarity','User']].to_csv(save+'.csv', index=False)\n",
    "        \n",
    "    return activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd77cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_data(core, crust, activities, data=[], save=False, restart=0):\n",
    "    #we assign core members as features\n",
    "    columns = ['user']+['user_'+str(n) for n in range(len(core))]\n",
    "    if save and not restart: \n",
    "        with open(save+'.csv', 'r') as save_file:\n",
    "            text = ['user'] + save_file.readlines()\n",
    "        for column in columns:\n",
    "            text[0] += ',' + str(column)\n",
    "        text[0] += '\\n'\n",
    "        with open(save+'.csv', 'a') as save_file:\n",
    "            save_file.writelines(text)\n",
    "        \n",
    "    #now we see which crust members are following which core members\n",
    "    for n, crust_member in [x for x in enumerate(crust) if x[0]>=restart]:\n",
    "        #this will hold what we find\n",
    "        next_line = [crust_member]\n",
    "        #look up who the crust member is following\n",
    "        following, pages = API.friends_ids(user_id=crust_member, cursor=-1)\n",
    "        \n",
    "        for core_member in core:\n",
    "            #page through if necessary\n",
    "            while pages[1] and (core_member not in following):\n",
    "                more_following, pages = API.friends_ids(user_id=crust_member, cursor=pages[1])\n",
    "                following += more_following\n",
    "            next_line.append(core_member in following)\n",
    "        \n",
    "        #We'll say people follow themselves\n",
    "        if n < len(core):\n",
    "            next_line[n]==True\n",
    "        \n",
    "        #now we store it\n",
    "        data.append(next_line)\n",
    "        if save: \n",
    "            with open(save+'.csv', 'r') as save_file:\n",
    "                text = save_file.readlines()\n",
    "            text[n+1] = text[n+1][:-1]\n",
    "            for datum in next_line:\n",
    "                text[n+1] += ',' + str(datum)\n",
    "            text[n+1] += '\\n'\n",
    "            with open(save+'.csv', 'w') as save_file:\n",
    "                save_file.writelines(text)\n",
    "        print('Stored data for member', n+1, 'of', len(crust))\n",
    "    \n",
    "    #and put it all together\n",
    "    df = pd.DataFrame(data, index=crust, columns=columns)\n",
    "    \n",
    "    #attach each user's average predicted polarity\n",
    "    averages = []\n",
    "    for user in network:\n",
    "        polarities = activities.Predicted_polarity[[i for i in activities.index if activities.User[i]==user]]\n",
    "        mean = (0 if len(polarities)==0 else np.mean(polarities))\n",
    "        averages.append(mean)\n",
    "    df['Average_polarity'] = averages\n",
    "    if save:\n",
    "        df.to_csv(save+'.csv', index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f69b3",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9224bc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925904609511690240 has mutuals [1175725425949073409, 925904609511690240]\n",
      "92508756 has mutuals [1151938716438552577, 92508756]\n",
      "834453600730484736 has mutuals [1149011056716500997, 834453600730484736]\n",
      "100554366 has mutuals [541774909, 100554366]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196290445 has mutuals [24183453, 196290445]\n",
      "252677295 has mutuals [18162640, 252677295]\n",
      "38720479 has mutuals [23627253, 38720479]\n"
     ]
    }
   ],
   "source": [
    "network = []\n",
    "network = build_network(center, breadth, depth, network, './'+name+'/network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1b28b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0\n",
      "Finished 10\n"
     ]
    }
   ],
   "source": [
    "activities=pd.DataFrame()\n",
    "activities=assemble_activities(network, activities, './'+name+'/activity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67f3ad3",
   "metadata": {},
   "source": [
    "This next one takes a while. Go play with Activity_investigator while you wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97c1f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored data for member 1 of 15\n",
      "Stored data for member 2 of 15\n",
      "Stored data for member 3 of 15\n",
      "Stored data for member 4 of 15\n",
      "Stored data for member 5 of 15\n",
      "Stored data for member 6 of 15\n",
      "Stored data for member 7 of 15\n",
      "Stored data for member 8 of 15\n",
      "Stored data for member 9 of 15\n",
      "Stored data for member 10 of 15\n",
      "Stored data for member 11 of 15\n",
      "Stored data for member 12 of 15\n",
      "Stored data for member 13 of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored data for member 14 of 15\n",
      "Stored data for member 15 of 15\n"
     ]
    }
   ],
   "source": [
    "with open('./'+name+'/network.csv', 'w+')as writer:\n",
    "    writer.writelines('\\n'.join([str(user) for user in network]))\n",
    "data = []\n",
    "df = frame_data(network, network, activities, data, './'+name+'/network')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d2c38",
   "metadata": {},
   "source": [
    "Now you can play with the rest in Network_investigator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af48df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
