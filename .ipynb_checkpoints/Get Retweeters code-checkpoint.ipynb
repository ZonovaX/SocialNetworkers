{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-4ef826150c55>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-4ef826150c55>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    bearer_token = #copy bearer_token here\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "bearer_token = #copy bearer_token here\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params):\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=params)\n",
    "    print(response.status_code)\n",
    "    while(response.status_code == 429):\n",
    "        time.sleep(6000)\n",
    "        print(response.status_code)\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    #print(response.text)\n",
    "    return response.json()\n",
    "\n",
    "def get_retweeters(id):#takes id of a twitter post\n",
    "    tweet_search_url = \"https://api.twitter.com/2/tweets/{}\".format(id)\n",
    "    headers = create_headers(bearer_token)\n",
    "    main_tweet_query_params = {'tweet.fields': 'created_at,author_id'}\n",
    "    main_tweet_json = connect_to_endpoint(tweet_search_url, headers, main_tweet_query_params)\n",
    "    main_tweet_text = main_tweet_json['data']['text']\n",
    "    main_tweet_author = main_tweet_json['data']['author_id']\n",
    "    \n",
    "    retweeters_id = []\n",
    "    all_search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "    retweeters_query_params = {'query': '\\\"{}\\\" retweets_of:{}'.format(main_tweet_text,main_tweet_author),'tweet.fields': 'author_id,created_at,public_metrics', 'max_results': '499', 'start_time': '2012-01-01T23:00:00Z'}\n",
    "    retweeters_json = connect_to_endpoint(all_search_url, headers, retweeters_query_params)\n",
    "    retweeters_id.append([tweet['author_id'] for tweet in retweeters_json['data']])\n",
    "    time.sleep(4)\n",
    "    while len(retweeters_json['meta']) > 3:\n",
    "        retweeters_query_params = {'query': '\\\"{}\\\" retweets_of:{}'.format(main_tweet_text,main_tweet_author),'tweet.fields': 'author_id,created_at,public_metrics', 'max_results': '499', 'start_time': '2012-01-01T23:00:00Z', 'next_token': '{}'.format(retweeters_json['meta']['next_token'])}\n",
    "        retweeters_json = connect_to_endpoint(all_search_url, headers, retweeters_query_params)\n",
    "        retweeters_id.append([tweet['author_id'] for tweet in retweeters_json['data']])\n",
    "        time.sleep(4)\n",
    "    flattened_retweeters_id = [id for page in retweeters_id for id in page]\n",
    "    return flattened_retweeters_id\n",
    "\n",
    "def follow_date(retweeter_id, figure_id): #gives date that retweeteter had retweeted the figure \"minretweets\" number of times\n",
    "    all_search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "    headers = create_headers(bearer_token)\n",
    "    \n",
    "    retweet_count = 0\n",
    "    end_year = 2013\n",
    "    min_retweets = 3 #consider them as starting following the figure after 3 retweets\n",
    "    date = ''\n",
    "    while retweet_count < min_retweets: \n",
    "        retweeters_query_params = {'query': 'retweets_of:{} from:{}'.format(figure_id,retweeter_id),'tweet.fields': 'author_id,created_at,public_metrics', 'max_results': '400', 'start_time': '2012-01-01T23:00:00Z', 'end_time': '{}-01-01T23:00:00Z'.format(str(end_year))}\n",
    "        retweeters_json = connect_to_endpoint(all_search_url, headers, retweeters_query_params)\n",
    "        if retweeters_json['meta']['result_count'] > 0:\n",
    "            retweet_count = retweeters_json['meta']['result_count']\n",
    "            date = retweeters_json['data'][-3]['created_at']\n",
    "            while len(retweeters_json['meta']) > 3: #paginating, running until the last page of retweets\n",
    "                retweeters_query_params = {'query': 'retweets_of:{} from:{}'.format(figure_id,retweeter_id),'tweet.fields': 'author_id,created_at,public_metrics', 'max_results': '400', 'start_time': '2012-01-01T23:00:00Z', 'end_time': '{}-01-01T23:00:00Z'.format(str(end_year)), 'next_token': '{}'.format(retweeters_json['meta']['next_token'])}\n",
    "                retweeters_json = connect_to_endpoint(all_search_url, headers, retweeters_query_params)\n",
    "                date = retweeters_json['data'][-3]['created_at']\n",
    "                time.sleep(4)\n",
    "        else: #if there are no results for a range of years, increase range of years by 1\n",
    "            end_year = end_year + 1\n",
    "            \n",
    "        time.sleep(4)\n",
    "    return date\n",
    "\n",
    "def get_user_activity(user_id, follow_date):#check the average user activity in a radius around the date the user starts following figure\n",
    "    all_search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "    headers = create_headers(bearer_token)\n",
    "    month_rad = 1#count number of tweets from follow_date - month_rad to follow_date + month_rad\n",
    "    \n",
    "    full_beginning_date_list = list(follow_date)\n",
    "    full_ending_date_list = list(follow_date)\n",
    "    extracted_date = follow_date[0:10]\n",
    "    \n",
    "    beginning_date = (date.fromisoformat(extracted_date) + relativedelta(months=-1*month_rad)).isoformat()\n",
    "    full_beginning_date_list[0:10] = list(beginning_date)\n",
    "    full_beginning_date_str = \"\".join(full_beginning_date_list)\n",
    "    ending_date = (date.fromisoformat(extracted_date) + relativedelta(months=month_rad)).isoformat()\n",
    "    full_ending_date_list[0:10] = list(ending_date)\n",
    "    full_ending_date_str = \"\".join(full_ending_date_list)\n",
    "    retweeters_query_params = {'query': 'from:{}'.format(user_id),'tweet.fields': 'author_id,created_at,public_metrics', 'max_results': '499', 'start_time': '{}'.format(full_beginning_date_str), 'end_time': '{}'.format(full_ending_date_str)}\n",
    "    retweeters_json = connect_to_endpoint(all_search_url, headers, retweeters_query_params)\n",
    "    tweets = []\n",
    "    tweets.append(retweeters_json['data'])\n",
    "    time.sleep(4)\n",
    "    while len(retweeters_json['meta']) > 3:\n",
    "        retweeters_query_params = {'query': 'from:{}'.format(user_id),'tweet.fields': 'author_id,created_at,public_metrics', 'max_results': '499', 'start_time': '{}'.format(full_beginning_date_str), 'end_time': '{}'.format(full_ending_date_str), 'next_token': '{}'.format(retweeters_json['meta']['next_token'])}\n",
    "        retweeters_json = connect_to_endpoint(all_search_url, headers, retweeters_query_params)\n",
    "        tweets.append(retweeters_json['data'])\n",
    "        time.sleep(4)\n",
    "    tweets_flat = [tweets for page in tweets for tweets in page]\n",
    "    total_tweets = len(tweets_flat)\n",
    "    return [total_tweets/(2*month_rad),tweets_flat]\n",
    "\n",
    "def get_monthly_tweets_list(user_id,full_date):\n",
    "    all_search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "    headers = create_headers(bearer_token)\n",
    "    all_tweets = []\n",
    "    num_Months_Before = 3\n",
    "    num_Months_After = 6\n",
    "    \n",
    "    for month in range(-1*num_Months_Before,num_Months_After):\n",
    "        full_beginning_date_list = list(full_date)\n",
    "        full_ending_date_list = list(full_date)\n",
    "        extracted_date = full_date[0:10]\n",
    "        \n",
    "        beginning_date = (date.fromisoformat(extracted_date) + relativedelta(months=month)).isoformat()\n",
    "        full_beginning_date_list[0:10] = list(beginning_date)\n",
    "        full_beginning_date_str = \"\".join(full_beginning_date_list)\n",
    "        \n",
    "        ending_date = (date.fromisoformat(extracted_date) + relativedelta(months=month+1)).isoformat()\n",
    "        full_ending_date_list[0:10] = list(ending_date)\n",
    "        full_ending_date_str = \"\".join(full_ending_date_list)\n",
    "        \n",
    "        retweeters_query_params = {'query': 'from:{}'.format(user_id),'tweet.fields': 'author_id,created_at,public_metrics', 'max_results': '499', 'start_time': '{}'.format(full_beginning_date_str), 'end_time': '{}'.format(full_ending_date_str)}\n",
    "        retweeters_json = connect_to_endpoint(all_search_url, headers, retweeters_query_params)\n",
    "        months_tweets = []\n",
    "        months_tweets.append(retweeters_json['data'])\n",
    "        time.sleep(4)\n",
    "        while len(retweeters_json['meta']) > 3:\n",
    "            retweeters_query_params = {'query': 'from:{}'.format(user_id),'tweet.fields': 'author_id,created_at,public_metrics', 'max_results': '499', 'start_time': '{}'.format(full_beginning_date_str), 'end_time': '{}'.format(full_ending_date_str), 'next_token': '{}'.format(retweeters_json['meta']['next_token'])}\n",
    "            retweeters_json = connect_to_endpoint(all_search_url, headers, retweeters_query_params)\n",
    "            months_tweets.append(retweeters_json['data'])\n",
    "            time.sleep(4)\n",
    "        monthly_tweets_flat = [tweets for page in months_tweets for tweets in page]\n",
    "        all_tweets.append(monthly_tweets_flat)\n",
    "        time.sleep(4)\n",
    "        \n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TL_tweet_id: '1023287366352396288' #id of specific tweet by TL\n",
    "TL_id: '468646961'\n",
    "TL_retweeters = get_retweeters(TL_tweet_id)\n",
    "TL_user_follow_DT = []\n",
    "#get follow date\n",
    "for retweeter in TL_retweeters:\n",
    "    follow_DT = follow_dates(retweeter, TL_id)\n",
    "    TL_user_follow_DT.append([retweeter, follow_DT])\n",
    "\n",
    "#check user activity\n",
    "for i in range(len(TL_user_follow_DT)):\n",
    "    activity = get_user_activity(TL_user_follow_DT[i][0],TL_user_follow_DT[i][1])\n",
    "    TL_user_follow_DT[i].append(activity[0])\n",
    "\n",
    "all_tweets = []\n",
    "for i in range(len(TL_user_follow_DT)):\n",
    "    if TL_user_follow_DT[i][2] > 15: #ie if they average more than 50 tweets a month\n",
    "        users_tweets = get_monthly_tweets_list([TL_user_follow_DT[0],TL_user_follow_DT[1]])\n",
    "        all_tweets.append(users_tweets)\n",
    "        all_tweets_df = pd.DataFrame(all_tweets)\n",
    "        all_tweets_df.to_csv(r'C:\\Users\\xzono\\Documents\\SocialNetworkers\\export_tweets.csv', index = False, Header = False)\n",
    "        \n",
    "all_tweets_df = pd.DataFrame(all_tweets)\n",
    "all_tweets_df.to_csv(r'C:\\Users\\xzono\\Documents\\SocialNetworkers\\export_tweets.csv', index = False, Header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-046b98b810ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\xzono\\Documents\\SocialNetworkers\\export_tweets.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHeader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "temp = [1,2,3,4]\n",
    "\n",
    "pd.DataFrame(temp).to_csv(r'C:\\Users\\xzono\\Documents\\SocialNetworkers\\export_tweets.csv', index = False, Header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
