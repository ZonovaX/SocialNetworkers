{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54509b1",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f18b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64096382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ysgard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#for cleaning data\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "punct=string.punctuation\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48d1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for making model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544437fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for saving the model\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b5e7f",
   "metadata": {},
   "source": [
    "Data (source: https://www.kaggle.com/kazanova/sentiment140?select=training.1600000.processed.noemoticon.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe5233f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>um</th>\n",
       "      <th>so-and-so</th>\n",
       "      <th>something</th>\n",
       "      <th>trash</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity          um                     so-and-so something  \\\n",
       "0               0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1               0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2               0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3               0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4               0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...           ...         ...                           ...       ...   \n",
       "1599995         4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996         4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997         4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998         4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999         4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   trash                                              tweet  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('../lotweets.csv', names=['polarity', 'um', 'so-and-so', 'something', 'trash', 'tweet'])\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccd9f3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n"
     ]
    }
   ],
   "source": [
    "#clean the text\n",
    "cleets = []\n",
    "for twittre in tweets.tweet:\n",
    "    #this removes mentions\n",
    "    twittre = re.sub(r'@[A-Za-z0-9_]+', '', twittre)\n",
    "    #this removes urls\n",
    "    twittre = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', twittre)        \n",
    "    #this removes everything but letters from the str\n",
    "    twittre = re.sub('[^a-zA-Z]',' ',twittre)\n",
    "    #this makes everything lowercase, then turns everything that is separated by a space into its own str\n",
    "    twittre = twittre.lower().split()\n",
    "    #this removes stop words and stems everything else\n",
    "    twittre = [stemmer.stem(word) for word in twittre if (word not in stop_words)]\n",
    "    # this puts it all back together with spaces inbetween\n",
    "    twittre = ' '.join(twittre)\n",
    "    cleets.append(twittre)\n",
    "    if not len(cleets)%10000:\n",
    "        print(len(cleets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3899511",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame({'Polarity':tweets.polarity, 'Tweet':cleets})\n",
    "tweets.to_csv('../lotwean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e3eb5",
   "metadata": {},
   "source": [
    "Cleaning words from https://www.kaggle.com/nltkdata/opinion-lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed44af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = pd.read_csv('../negative-words.txt', names=['words'])\n",
    "positive = pd.read_csv('../positive-words.txt', names=['words'])\n",
    "cleets = []\n",
    "for twittre in negative.words:\n",
    "    #this removes mentions\n",
    "    twittre = re.sub(r'@[A-Za-z0-9_]+', '', twittre)\n",
    "    #this removes urls\n",
    "    twittre = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', twittre)        \n",
    "    #this removes everything but letters from the str\n",
    "    twittre = re.sub('[^a-zA-Z]',' ',twittre)\n",
    "    #this makes everything lowercase, then turns everything that is separated by a space into its own str\n",
    "    twittre = twittre.lower().split()\n",
    "    #this removes stop words and stems everything else\n",
    "    twittre = [stemmer.stem(word) for word in twittre if (word not in stop_words)]\n",
    "    # this puts it all back together with spaces inbetween\n",
    "    twittre = ' '.join(twittre)\n",
    "    cleets.append(twittre)\n",
    "negative = set(cleets)\n",
    "with open('../negatean.txt', 'w+') as writer:\n",
    "    for word in negative:\n",
    "        writer.write(word+'\\n')\n",
    "\n",
    "        \n",
    "cleets = []\n",
    "for twittre in positive.words:\n",
    "    #this removes mentions\n",
    "    twittre = re.sub(r'@[A-Za-z0-9_]+', '', twittre)\n",
    "    #this removes urls\n",
    "    twittre = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', twittre)        \n",
    "    #this removes everything but letters from the str\n",
    "    twittre = re.sub('[^a-zA-Z]',' ',twittre)\n",
    "    #this makes everything lowercase, then turns everything that is separated by a space into its own str\n",
    "    twittre = twittre.lower().split()\n",
    "    #this removes stop words and stems everything else\n",
    "    twittre = [stemmer.stem(word) for word in twittre if (word not in stop_words)]\n",
    "    # this puts it all back together with spaces inbetween\n",
    "    twittre = ' '.join(twittre)\n",
    "    cleets.append(twittre)\n",
    "positive = set(cleets)\n",
    "with open('../positivean.txt', 'w+') as writer:\n",
    "    for word in positive:\n",
    "        writer.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc643d0",
   "metadata": {},
   "source": [
    "Removing unnecessary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "478d4fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n"
     ]
    }
   ],
   "source": [
    "negative = set(pd.read_csv('../negativean.txt', names=['words'])['words'])\n",
    "positive = set(pd.read_csv('../positivean.txt', names=['words'])['words'])\n",
    "tweets = pd.read_csv('../lotwean.csv')\n",
    "negativean, positivean = set([]), set([])\n",
    "for n,tweet in enumerate(tweets.Tweet):\n",
    "    negatweet = negative.intersection(set(tweet.split()))\n",
    "    positweet = positive.intersection(set(tweet.split()))\n",
    "    negativean = negativean.union(negatweet)    \n",
    "    positivean = positivean.union(positweet)\n",
    "    if not n%10000:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "707e385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = negativean.union(positivean)\n",
    "tweets['Twords'] = [words.intersection(set(tweet.split())) for tweet in tweets.Tweet]\n",
    "tweets['Nords'] = [len(tword)!=0 for tword in tweets.Twords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb5d12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[['Polarity', 'Tweet', 'Twords']].to_csv('../lotwean.csv')\n",
    "tweets['keep'] = [int(tword=='set()') for tword in tweets.Twords]\n",
    "tweets[['Polarity', 'Tweet', 'drop']].to_csv('../tweetdb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1bb41",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a26d6bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Polarity\n",
       "0    31462\n",
       "4    29846\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keepercentage = .05\n",
    "\n",
    "negative = set(pd.read_csv('../negativean.txt', names=['words'])['words'])\n",
    "positive = set(pd.read_csv('../positivean.txt', names=['words'])['words'])\n",
    "words = list(negative.union(positive))\n",
    "\n",
    "tweets = pd.read_csv('../tweetdb.csv')\n",
    "tweets = tweets.drop(index=[i for i in tweets.index if tweets.Drop[i]])\n",
    "tweets = tweets.sample(frac=keepercentage, random_state=347)\n",
    "tweets.value_counts('Polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f63bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectord = CountVectorizer()\n",
    "vectord.fit(words)\n",
    "X = vectord.transform(tweets.Tweet).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2400d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73      4495\n",
      "           4       0.72      0.74      0.73      4264\n",
      "\n",
      "    accuracy                           0.73      8759\n",
      "   macro avg       0.73      0.73      0.73      8759\n",
      "weighted avg       0.73      0.73      0.73      8759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X, tweets.Polarity,\n",
    "                                                 test_size=2/7, stratify=tweets.Polarity,\n",
    "                                                 shuffle=True, random_state=347)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da363918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(vectord, '../vectord.joblib') \n",
    "dump(model, '../model.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe6e5b",
   "metadata": {},
   "source": [
    "The sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b691f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "vectord = load('../vectord.joblib') \n",
    "model = load('../model.joblib') \n",
    "\n",
    "#tweets must be cleaned first\n",
    "def sentiment(tweets, analogue=False):\n",
    "    if analogue: \n",
    "        probs = model.predict_proba(vectord.transform(tweets).toarray())\n",
    "        return probs[:,1]-probs[:,0]\n",
    "    else: return model.predict(vectord.transform(tweets).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77770ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.78      0.71     55767\n",
      "           4       0.72      0.58      0.64     55658\n",
      "\n",
      "    accuracy                           0.68    111425\n",
      "   macro avg       0.68      0.68      0.67    111425\n",
      "weighted avg       0.68      0.68      0.67    111425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('../tweetdb.csv').sample(frac=.1, random_state=343)\n",
    "print(classification_report(tweets.Polarity, sentiment(tweets.Tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6612bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
